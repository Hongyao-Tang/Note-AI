---
author: "Hongyao Tang"
title: "2. OS Optimization"
date: "2025-07-02"
description: "Optimization for Linux OS"
tags: [
    "optimization",
]
ShowToc: true
weight: 2
---


## Orchestrator

#### Job scheduler
- SLURM is deployed for training clusters 
- Kubernetes is typically favored for inference clusters
- Open-source Slinky project and CoreWeaveâ€™s SUNK product are examples of these integrated solutions to simplify cluster management across training and inference workloads
  
#### S - HW-topology-unaware scheduling
- Pod placements. The requirement is allocating resources to containers in a manner that is aware of the hardware topology including the NUMA node and network bandwidth configurations
- Kubernetes is not topology aware. It treats each GPU as a resource but doesnâ€™t know if GPU0 and GPU1 are on the same NUMA node or if they use the same NVLink interconnect.

T - Use node labels to explicitly mark GPU or node topology.

- Ensures that multi-GPU pods are placed on hosts where the GPUs share the same NVLink domain or NUMA node.

T - Kubernetes Topology Manager and Nvidiaâ€™s hardware-aware GPU Operator **device plugin** can provide detailed topology information. 

- they can detect that GPU 0 is connected to NUMA node 0, NVLink domain A, and PCIe bus Z.
- Kubernetes scheduler can then use this information to allocate containers to GPUs



A

- simply request a GPU in your Kubernetes pod specification
- device plugin mounts the /dev/nvidia* devices for you
- device plugin also mounts the driver libraries from the host into the container.


R

ensure optimal performance by minimizing cross-NUMA-node communication and reducing latency in multi-node GPU workloads.


#### S - Resource contention for performance-sensitive Kubernetes jobs

| èµ„æºç±»å‹ | æ˜¯å¦å¯å‹ç¼© | è¯´æ˜ |
|----------|-------------|------|
| CPU | âœ… å¯å‹ç¼© | CPU ä½¿ç”¨å¯ä»¥è¢«é™åˆ¶ï¼ˆthrottleï¼‰ï¼ŒPod ä¼šå˜æ…¢ä½†ä¸ä¼šè¢«æ€æ­»ã€‚ |
| Memory | âŒ ä¸å¯å‹ç¼© | å†…å­˜ä½¿ç”¨è¶…å‡ºé™åˆ¶ä¼šå¯¼è‡´ OOMï¼ˆOut of Memoryï¼‰å¹¶è¢«æ€æ­»ã€‚ |
- a greedy container, if unconstrained, could allocate too much memory on the host. This would cause the host to swap some of its memory to disk.
- If an unbounded container uses too much memory on the host, the infamous Linux â€œOOM Killerâ€ will start killing processes - and potentially your Kubernetes job - even if your job wasnâ€™t the one using too much memory.
- The OOM killer uses heuristics when deciding which pods to kill. Sometimes it decides to kill the largest running pod which is likely your large training or inference job holding lots of data in CPU RAM to feed the GPUs.



T - Guaranteed QoS

Kubernetesæ ¹æ®**Podä¸­å®¹å™¨**çš„èµ„æºè¯·æ±‚å’Œé™åˆ¶ï¼ˆ`requests` å’Œ `limits`ï¼‰å°†å…¶åˆ†ä¸ºä¸‰ç§ QoS ç±»åˆ«ï¼š
| QoS ç±»åˆ« | æ¡ä»¶ | ç‰¹ç‚¹ |
|----------|------|------|
| **Guaranteed** | æ‰€æœ‰å®¹å™¨çš„ `requests` å’Œ `limits` éƒ½è®¾ç½®äº†ï¼Œå¹¶ä¸”å€¼ç›¸ç­‰ | æœ€ç¨³å®šï¼Œä¼˜å…ˆçº§æœ€é«˜ï¼Œæœ€ä¸å®¹æ˜“è¢«é©±é€ |
| **Burstable** | è‡³å°‘ä¸€ä¸ªå®¹å™¨è®¾ç½®äº† `requests`ï¼Œä½† `requests` å’Œ `limits` ä¸å®Œå…¨ç›¸ç­‰ | ä¸­ç­‰ä¼˜å…ˆçº§ï¼Œèµ„æºä½¿ç”¨çµæ´» |
| **BestEffort** | æ‰€æœ‰å®¹å™¨éƒ½æ²¡æœ‰è®¾ç½® `requests` å’Œ `limits` | ä¼˜å…ˆçº§æœ€ä½ï¼Œæœ€å®¹æ˜“è¢«é©±é€ |


T - Exclusively, request all of the CPUs and GPUs of a given node so that nothing else interferes or contends with the jobsâ€™ resources.


## Container
### Image
#### S -  â€œbut it works on my machineâ€ problem
T - dependency as readonly layer

- include CUDA lib
- making sure that the CUDA libraries inside the container match the driver on the host.


R

dependencies versions are consistent


#### S - Large image size slow container startup
- Container startup times can be quite a bit slower if the image is huge and needs to be pulled over the network. 

T

- Maultistage build: remove build tool and tem build file
- Multi-layer build: to share layers with ohter images

R

saves disk space and improves container startup time.



### Overlay to host
#### Why use container not VM?
In contrast to VMâ€™s, containers share the host OS kernel, has near bare metal performance
- CPU, memory, FS, network operations perform at near-native speed
- directly use the hostâ€™s GPU driver and hardware
  - å®¹å™¨ â†’ Host OS â†’ é©±åŠ¨ â†’ ç¡¬ä»¶
  - Guest OS â†’ è™šæ‹ŸåŒ–å±‚ï¼ˆhypervisorï¼‰â†’ Host OS â†’ é©±åŠ¨ â†’ ç¡¬ä»¶



#### S - [FS] Overlay(to host) filesystem has overhead when IO with disk
- The main difference when running in a Docker container versus running directly on the host might be in I/O.
- There is some overhead when using an overlay filesystem
  - This extra latency arises because the filesystem must check multiple underlying layers - both read-only and writable - to determine which version of a file should be returned.
  - Furthermore, when writing, the copy-on-write (CoW) mechanism used by the overlay. 
- But model training often involves heavy I/O operations when reading datasets, loading a model, and writing model checkpoints re. the FS.

t - Mount a host/network directory into the container using bind mounts.
- inpout data/model directory
- output checkppint directory


A

-v /data/dataset:/mnt/dataset:ro

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: hostpath-demo
spec:
  containers:
  - name: busybox
    image: busybox
    command: [ "sleep", "3600" ]
    volumeMounts:
    - name: host-volume
      mountPath: /data/host
  volumes:
  - name: host-volume
    hostPath:
      path: /data/on/host
      type: Directory
```
- å®¿ä¸»æœºç›®å½•ï¼ˆhostPathï¼‰ä¸­å·²æœ‰æ•°æ®
- å®¹å™¨ä¸­æŒ‚è½½ç‚¹ï¼ˆmountPathï¼‰ä¹Ÿå·²æœ‰æ•°æ®ï¼ˆæ¯”å¦‚é•œåƒä¸­é¢„ç½®çš„æ–‡ä»¶ï¼‰
- å®¹å™¨å¯åŠ¨åï¼Œå®¿ä¸»æœºç›®å½•ä¼šè¦†ç›–å®¹å™¨å†…åŸæœ‰ç›®å½•
- å®¹å™¨ä¸­åŸæœ¬åœ¨è¯¥è·¯å¾„ä¸‹çš„æ–‡ä»¶å°†ä¸å¯è§ï¼Œä½†ä¸ä¼šè¢«åˆ é™¤ï¼ˆåªæ˜¯è¢«æŒ‚è½½å±‚éšè—ï¼‰

R

ensure that I/O is not bottlenecked by the overhead of the containerâ€™s copy-on-write mechanism.



#### S - [Net] Overlay(to host) network/NAT has overhead when communicating internally and externally
```
Pod â†â†’ Podï¼šé€šè¿‡ Overlay ç½‘ç»œé€šä¿¡ï¼ˆå¯èƒ½å°è£…ï¼Œä½†ä¸ NATï¼‰
Pod â†’ å¤–éƒ¨ï¼šé€šè¿‡ SNATï¼ˆæºåœ°å€è½¬æ¢ï¼‰
å¤–éƒ¨ â†’ Podï¼šé€šè¿‡ DNATï¼ˆç›®æ ‡åœ°å€è½¬æ¢ï¼‰
```
- When you run multi-node GPU workloads using containers with Kubernetes, the pods need to talk to each other.
- In Kubernetes, by default pods have their own IP and there might be an overlay network or network-address translation (NAT) between pods on different nodes. 
- This can introduce complications and additional overhead.

T - Use hots network

- containerâ€™s network is not isolated as it uses the hostâ€™s network interface directly.

A

--network=host

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: hostnetwork-demo
spec:
  hostNetwork: true
  containers:
  - name: nginx
    image: nginx
    ports:
    - containerPort: 80
```


R

Using host networking allows a container to access the InfiniBand interconnect exactly as the host does - without any additional translation or firewall layers


## OS - Pin to dedication
OSçš„è®¾è®¡åŸæœ¬æ˜¯ä¸ºäº†å¤šä»»åŠ¡çš„äº¤æ›¿ï¼ŒMLéœ€è¦ä¸“ä¸€æé«˜æ€§èƒ½ï¼Œæ‰€æœ‰è¦Pinä½æŸäº›äº¤æ›¿æœºåˆ¶ï¼Œä¿æŒç¨³å®šï¼Œå‡å°‘overhead

### CPU for GPU
#### Why don't GPUs just replace CPUs? 
- Each has its talent
- Formula-1 race cars and Dump trucks both exist at the same time because they're designed for different jobs.
- GPUs are extremely good at doing a very narrow set of instructions
  - namely the type of instructions needed to calculate the pixels of an image.
- Wheras a CPU is kinda good at doing a vast, vast range of possible instructions/calculations, including those that GPUs would really struggle with.
  - complex instruction sets, data processing in RAM, I/O operations, and operating system management

#### S - [ç»‘å®šthreadåˆ°ä¸“ç”¨CPU]Thread is interrupted by os scheduler
- a very latency-sensitive thread that feeds the GPU with data
- Linux by default uses the Completely Fair Scheduler (CFS), impacting the thread

T - Use real-time first-in-first-out (FIFO) or round-robin (RR) scheduling for that thread


å®æ—¶ï¼ˆReal-Timeï¼‰åœ¨è®¡ç®—æœºç³»ç»Ÿä¸­ï¼Œç³»ç»Ÿä¸ä»…è¦æ­£ç¡®åœ°å®Œæˆä»»åŠ¡ï¼Œè¿˜è¦åœ¨è§„å®šçš„æ—¶é—´å†…å®Œæˆã€‚å†è§„å®šçš„æ—¶é—´å†…å®Œæˆï¼Œä¸æ˜¯æ— é™åˆ¶ï¼Œæ„Ÿè§‰åƒæ˜¯å®æ—¶å®Œæˆçš„
- ç¡¬å®æ—¶ï¼šé£æœºèµ·é£æ—¶ï¼Œä¼ æ„Ÿå™¨å¿…é¡»åœ¨ 10 æ¯«ç§’å†…å“åº”ï¼Œå¦åˆ™å¯èƒ½å¯¼è‡´äº‹æ•…ã€‚
- è½¯å®æ—¶ï¼šè§†é¢‘æ’­æ”¾å™¨å¦‚æœå¶å°”å¡é¡¿ 0.1 ç§’ï¼Œç”¨æˆ·å¯èƒ½æ„Ÿè§‰ä¸èˆ’æœï¼Œä½†ä¸ä¼šå‡ºé”™ã€‚
- éå®æ—¶ï¼šä½ æ‰“å¼€ä¸€ä¸ªç½‘é¡µï¼Œç­‰ 3 ç§’ä¹Ÿæ²¡å…³ç³»ï¼Œåªæ˜¯ä½“éªŒå·®ä¸€ç‚¹ã€‚


å¯¹æ¯”
| ç‰¹æ€§ | CFSï¼ˆCompletely Fair Schedulerï¼‰ | FIFOï¼ˆFirst-In-First-Outï¼‰ | RRï¼ˆRound-Robinï¼‰ |
|------|----------------------------------|-----------------------------|--------------------|
| **è°ƒåº¦ç±»å‹** | éå®æ—¶è°ƒåº¦å™¨ | å®æ—¶è°ƒåº¦å™¨ | å®æ—¶è°ƒåº¦å™¨ |
| **è°ƒåº¦ç­–ç•¥** | åŸºäºè™šæ‹Ÿè¿è¡Œæ—¶é—´ï¼ˆvruntimeï¼‰å…¬å¹³è°ƒåº¦ | æŒ‰ä¼˜å…ˆçº§ + åˆ°è¾¾é¡ºåºæ‰§è¡Œ | æŒ‰ä¼˜å…ˆçº§ + æ—¶é—´ç‰‡è½®è½¬ |
| **ä¼˜å…ˆçº§æœºåˆ¶** | nice å€¼ï¼ˆ-20 ~ +19ï¼‰ | å®æ—¶ä¼˜å…ˆçº§ï¼ˆ1~99ï¼‰ | å®æ—¶ä¼˜å…ˆçº§ï¼ˆ1~99ï¼‰ |
| **æ—¶é—´ç‰‡æ”¯æŒ** | âœ… åŠ¨æ€åˆ†é… | âŒ æ— æ—¶é—´ç‰‡ | âœ… å›ºå®šæ—¶é—´ç‰‡ï¼ˆå¦‚ 100msï¼‰ |
| **æŠ¢å æœºåˆ¶** | âœ… **æ”¯æŒï¼ˆä½ä¼˜å…ˆçº§è¢«æŠ¢å ï¼‰**| âœ… **ä»…è¢«æ›´é«˜ä¼˜å…ˆçº§æŠ¢å **| âœ… **åŒ FIFOï¼Œä½†åŒä¼˜å…ˆçº§è½®è½¬** |
| **å…¬å¹³æ€§** | âœ… é«˜ï¼ˆåŸºäºè¿è¡Œæ—¶é—´ï¼‰ | âŒ æ— ï¼ˆå…ˆæ¥å…ˆæœåŠ¡ï¼‰ | âœ… åŒä¼˜å…ˆçº§å…¬å¹³è½®è½¬ |
| **é¥¥é¥¿é£é™©** | âœ… æœ‰ï¼ˆä½ nice å€¼è¿›ç¨‹å¯èƒ½é¥¿æ­»ï¼‰ | âœ… æœ‰ï¼ˆä½ä¼˜å…ˆçº§å¯èƒ½æ°¸è¿œä¸è¿è¡Œï¼‰ | âœ… æœ‰ï¼ˆä½ä¼˜å…ˆçº§å¯èƒ½é¥¿æ­»ï¼‰ |
| **é€‚ç”¨åœºæ™¯** | æ™®é€šç”¨æˆ·è¿›ç¨‹ã€åå°ä»»åŠ¡ | ç¡¬å®æ—¶ä»»åŠ¡ã€é©±åŠ¨æ§åˆ¶ | å¤šä¸ªå®æ—¶ä»»åŠ¡å…±äº« CPU |
| **å†…æ ¸æ¥å£** | `SCHED_NORMAL` / `SCHED_OTHER` | `SCHED_FIFO` | `SCHED_RR` |
| **å…¸å‹åº”ç”¨** | æµè§ˆå™¨ã€æ•°æ®åº“ã€Web æœåŠ¡ | é£æ§ç³»ç»Ÿã€å·¥ä¸šæ§åˆ¶ | å®æ—¶éŸ³é¢‘ã€æœºå™¨äººæ§åˆ¶ |


A

1.Pod é…ç½®ä¸­å¯ç”¨ hostPID å’Œ hostNetworkï¼ˆå¯é€‰ï¼‰
- hostPID: true å…è®¸å®¹å™¨çœ‹åˆ°å®¿ä¸»æœºçš„è¿›ç¨‹ IDï¼Œä¾¿äºè°ƒè¯•å’Œè®¾ç½®è°ƒåº¦ç­–ç•¥
```yaml
spec:
  hostPID: true
  hostNetwork: true
  containers:
  - name: ml-worker
    image: your-ml-image
    securityContext:
      xxx
```

2.æˆäºˆ CAP_SYS_NICE æƒé™
- å…è®¸å®¹å™¨å†…çš„è¿›ç¨‹ä½¿ç”¨ sched_setscheduler() è®¾ç½®ä¸º SCHED_FIFO
```yaml
securityContext:
  capabilities:
    add: ["SYS_NICE"]
```

3.åœ¨å®¹å™¨ä¸­è®¾ç½®è°ƒåº¦ç­–ç•¥
- å¯ä»¥åœ¨å®¹å™¨å¯åŠ¨è„šæœ¬æˆ–ç¨‹åºä¸­ä½¿ç”¨ chrt å‘½ä»¤æˆ– C è¯­è¨€ API è®¾ç½®è°ƒåº¦ç­–ç•¥
  - -f è¡¨ç¤º FIFO
  - 80 æ˜¯å®æ—¶ä¼˜å…ˆçº§ï¼ˆ1~99ï¼‰
```bash
chrt -f 80 ./your_ml_program
```

4.ç¡®ä¿å®¿ä¸»æœºå…è®¸å®æ—¶è°ƒåº¦
```bash
# systemd
[Service]
LimitRTPRIO=99
```

5.summary
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: fifo-ml-pod
spec:
  hostPID: true
  hostNetwork: true
  containers:
  - name: ml
    image: your-ml-image
    securityContext:
      capabilities:
        add: ["SYS_NICE"]
    command: ["chrt", "-f", "80", "./your_ml_program"]
```


R

- ensure the runs without being preempted by normal threads

ç¼ºç‚¹
- However, use this with caution as real-time threads can starve other processes if not managed properly. 
- In practice, however, if youâ€™ve pinned your threads to dedicated cores, you often donâ€™t need to mess with real-time thread priorities, but itâ€™s worth keeping an eye on.


T2 - isolate cores entirely for your process from scheduler

A2

1.Defining isolated CPUs
```bash
# /sys/devices/system/cpu/isolated
isolcpus=<cpu number>,....,<cpu number>
```

2.assigning processes/tasks to or from the CPU
```bash
# taskset command can be used to set the CPU affinity of processe
taskset -c <CPU NUMBER> -p <PID>

# cset command allows you to group CPUs and memory into logical entities and restrict processes to the resources of one or more of those logical entities.
cset set --cpu <CPU> <CPUSET NAME> # åˆ›å»º CPU é›†åˆ
cset proc --set=rt --exec ./your_program # å¯åŠ¨è¿›ç¨‹å¹¶ç»‘å®š
cset proc --move --pid=<PID>,...,<PID> --toset=<CPUSET NAME> # ç»‘å®šå·²æœ‰è¿›ç¨‹
taskset -cp <PID> # éªŒè¯ç»‘å®šæ•ˆæœ
```

R2

OS scheduler leaves those CPU cores for you to use as your program wishes.



#### S - [pin cpu,ç»‘å®šthreadåˆ°ç‰¹å®šCPU]Bind thread to a NUMA node of CPU and memory
- CPU-based worker processes - CPU is responsible for preparing the next batch of data including loading the data from disk, tokenizing the data, transforming it, etc.
- by default, the Linux scheduler will not use a NUMA-aware scheduling algorithm.

T - NUMA node

- A NUMA node is a logical grouping of CPUs, GPUs, NICs, and memory that are physically close to each other.
- Accessing resources within a single NUMA node is faster than accessing resources in other NUMA nodes.

A

```bash
# 1.æŸ¥çœ‹è¯¦ç»†ä¿¡æ¯
numactl -H
# available: 2 nodes (0-1)
# node 0 cpus: 0 1 2 ... 15
# node 0 size: 128000 MB
# node 1 cpus: 16 17 ... 31
# node 1 size: 128000 MB

# 2.NUMA èŠ‚ç‚¹æ˜¯ç”± ä¸»æ¿å’Œ CPU æ¶æ„å†³å®šçš„ï¼Œä¸èƒ½é€šè¿‡è½¯ä»¶åˆ›å»ºæ–°çš„ NUMA èŠ‚ç‚¹ã€‚ä½ åªèƒ½ï¼š


# 3.å°†è¿›ç¨‹/çº¿ç¨‹åˆ†é…åˆ°ç‰¹å®š NUMA èŠ‚ç‚¹
numactl --cpubind=0 --membind=0 python train.py
# --cpubind=<node>	# å°†è¿›ç¨‹ç»‘å®šåˆ°æŒ‡å®š NUMA èŠ‚ç‚¹çš„ CPU ä¸Š
# --membind=<node>	# å°†è¿›ç¨‹ä½¿ç”¨çš„å†…å­˜é™åˆ¶åœ¨æŒ‡å®š NUMA èŠ‚ç‚¹

# 4.æŸ¥çœ‹å†…å­˜è®¿é—®
numastat -p $(pidof python)
```

```py
import numa
numa.available_nodes()Â  # æŸ¥çœ‹å¯ç”¨èŠ‚ç‚¹
numa.bind(0)Â Â Â Â Â Â Â Â Â Â Â  # ç»‘å®šåˆ°èŠ‚ç‚¹ 0
```

#### S - [ç»‘å®šç¡¬ä»¶ä¸­æ–­åˆ°ç‰¹å®šCPU]Bind hardware interrupts to a NUMA node of CPU
- If your GPU or NIC running in a NUMA node 0 generates hardware interrupts, youâ€™d like those to be handled by a core on the same NUMA node. 
- Otherwise, a random core from another NUMA node has to communicate across NUMA nodes.
- This could evict useful cache data on the other NUMA node.

T - bind hardware interrupts to specific cores in a NUMA-aware manner


A

irqbalance ä¼šè¯»å–ç³»ç»Ÿçš„ NUMA æ‹“æ‰‘ï¼ˆé€šè¿‡ /sys å’Œ ACPI è¡¨ï¼‰ï¼Œå¹¶å°è¯•å°†è®¾å¤‡çš„ä¸­æ–­åˆ†é…åˆ°ä¸è®¾å¤‡åœ¨åŒä¸€ NUMA èŠ‚ç‚¹çš„ CPU ä¸Šã€‚
```bash
systemctl start irqbalance
```


cat /proc/interrupts

R

Avoid cross-NUMA-node interrupts that could evict useful cache data on the other NUMA node.


#### S - CPU dwonclock or sleep

| åˆ†ç±»é¡¹   | âš¡ P-stateï¼ˆPerformance Stateæ€§èƒ½çŠ¶æ€ï¼‰                                       | ğŸ˜´ C-stateï¼ˆIdle Stateç©ºé—²çŠ¶æ€ï¼‰                                             |
|----------|--------------------------------------------------------------|---------------------------------------------------------------------|
| å®šä¹‰     | è¿è¡Œæ—¶é€šè¿‡è°ƒæ•´é¢‘ç‡å’Œç”µå‹æ¥èŠ‚èƒ½                               | CPU æ ¸å¿ƒç©ºé—²æ—¶è¿›å…¥ä¸åŒç¡çœ çº§åˆ«                                     |
| ç‰¹ç‚¹     | - P0 æ˜¯æœ€é«˜æ€§èƒ½çŠ¶æ€<br>- P1ã€P2â€¦ æ˜¯é€æ¸é™ä½é¢‘ç‡å’Œç”µå‹çš„çŠ¶æ€<br>- ç”±æ“ä½œç³»ç»Ÿæˆ–ç¡¬ä»¶åŠ¨æ€è°ƒæ•´ï¼ˆDVFSï¼‰ | - C0ï¼šæ´»è·ƒçŠ¶æ€<br>- C1ï¼šè½»åº¦ç¡çœ ï¼Œå¿«é€Ÿå”¤é†’<br>- C6ï¼šæ·±åº¦ç¡çœ ï¼Œå‡ ä¹æ–­ç”µï¼Œå”¤é†’æ…¢ |
| å…³é”®è¯   | downclockï¼ˆé™é¢‘ï¼‰                                            | sleepï¼ˆç¡çœ ï¼‰                                                      |
- many compute nodes will run CPUs in a power-saving mode which 
  - either downclocks a CPU 
  - or puts it to sleep when itâ€™s idle
- This helps save energy, reduce heat, and lower cost. 
- tThese power management features could cause extra latency when the system wakes the CPUs up again when new work arrives.
  â—‹ Bubbles are periods of time when the GPU is waiting for the CPU to resume data processing.


T -  trade a bit of extra power draw for more responsive CPU behavior.

- configure the CPU frequency governor to â€œperformanceâ€ mode which keeps the CPU at max frequency all the time. 
- disabling deep C-states can keep cores from going into a low-power sleep state

A
```bash
cpupower frequency-set -g performance


# /etc/default/grub
# åœ¨ GRUB_CMDLINE_LINUX_DEFAULT ä¸­æ·»åŠ ï¼š
intel_idle.max_cstate=0 processor.max_cstate=1
update-grub
reboot
```


R

- Avoid bubbles/reduce hiccups
- To keep GPU fed all the potential time
- For maximum and consistent performance



### Memory for GPU
#### S - [Pageçš„å•ä½]Large data may have many pages if using small page size
- big-memory workloads - when you have processes using tens or hundreds of gigabytes of memory
- Linux memory management typically uses 4 KB pages
- managing millions of tiny pages is inefficient


T - Transparent Huge Pages (THP) 2 MB (default) or even 1 GB pages

A

```bash
# å¯ç”¨ç³»ç»ŸèŒƒå›´å†…çš„ THP
echo always | sudo tee /sys/kernel/mm/transparent_hugepage/enabled
# - alwaysï¼šç³»ç»Ÿä¼šå°½å¯èƒ½ä½¿ç”¨ THP
# - madviseï¼šä»…åœ¨åº”ç”¨ç¨‹åºä½¿ç”¨ madvise() æ ‡è®°çš„å†…å­˜åŒºåŸŸå¯ç”¨ THPï¼ˆæ¨èï¼‰
# - neverï¼šå®Œå…¨ç¦ç”¨ THP
```

R

Reduce the overhead of virtual memory management
- fewer page faults, a few percent improvement in throughput
- less pressure on the Translation Lookaside Buffer (TLB).
  - The TLB is a cache that the CPU uses to map virtual addresses to physical ones. 
  - Fewer, larger pages means the TLB can cover more memory with the same number of entries


#### S - [pin mem, Pageçš„äº¤æ¢]Page swapping impact data transferring to GPU
- When transferring/copy data from CPU to the GPU
- Normally, the OS can decide to swap memory pages in and out - or move them around as needed.

T - page pinning/page locking/memory pinning to ensure pages in RAM not disk

- if you allocate pinned memory, the OS guarantees those memory pages will stay in physical RAM and not be swapped out or moved

A

1.The OS has a limit on how much memory a user can lock (pin). Typically, one sets it to unlimited for large AI workloads and HPC applications.
```bash
ulimit -l <max locked memory in KB>
ulimit -l 65536  # è®¾ç½®ä¸º 64MB
```

2.tells Linux to avoid swapping except under extreme memory pressure.
```bash
/etc/sysctl.conf
vm.swappiness = 0

sysctl -p
```
- PyTorchâ€™s DataLoader has a flag pin_memory=True which, when true, means the batches loaded will be placed in pinned RAM.
```py
from torch.utils.data import DataLoader
from torchvision import datasets, transforms

# å®šä¹‰æ•°æ®é›†
transform = transforms.ToTensor()
dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)

# åˆ›å»º DataLoaderï¼Œå¯ç”¨ pin_memory
dataloader = DataLoader(dataset, batch_size=64, shuffle=True, num_workers=4, pin_memory=True)
```


R

pinned (page-locked) host memory can dramatically improve throughput.
- Copying from pinned host CPU memory to a GPU is often 2â€“3Ã— faster than from regular pageable CPU memory.


### FS
#### S - OS FS page cache/flush too small/freuqent for large data write-back
- Write frequent checkpoints to disk in case you need to restart a failed job from a known good checkpoint
- During checkpointing, however, huge bursts of data might fill up the OS page cache and cause stalls.

T -  allow a lot of dirty cache to accumulate and flush in the background

A

vm.dirty_ratio - å½“è„é¡µå ç”¨å†…å­˜è¾¾åˆ°è¯¥æ¯”ä¾‹æ—¶ï¼Œå†™å…¥æ•°æ®çš„è¿›ç¨‹ä¼šè¢«é˜»å¡ï¼Œç›´åˆ°è„é¡µè¢«å†™å…¥ç£ç›˜
vm.dirty_background_ratio - å½“ç³»ç»Ÿä¸­è„é¡µå ç”¨å†…å­˜è¾¾åˆ°è¯¥æ¯”ä¾‹æ—¶ï¼Œåå°çº¿ç¨‹å¼€å§‹å¼‚æ­¥å†™å…¥ç£ç›˜
```bash
 /etc/sysctl.conf
vm.dirty_background_ratio = 10
vm.dirty_ratio = 20

sysctl -p
```

R

Training process doesnâ€™t block on file writes


### GPU itself (Driver)
#### S - How OS to interface with GPUs HW
- manages low-level GPU operations including memory allocation on the device, task scheduling on GPU cores, and partitioning the GPU for multi-tenant usage.

T - driver

NVIDIA driver will
- install kernel modules
- create device files like /dev/nvidia0

Tools like nvidia-smi come with the driver 
- allow you to monitor temperatures, measure utilization, query error-correcting code (ECC) memory status, and enable different GPU modes like persistence mode.






- Normally, when multiple processes share a single GPU, the GPUâ€™s scheduler time-slices between them. 
- If those kernels are short and thereâ€™s an idle gap between them, the GPU can end up underutilized as itâ€™s 
  - doing â€œping-pongâ€ context switches and
  - not overlapping the work.


T1 - Umbrella with NVIDIAâ€™s Multi-Process Service (MPS) 
- A feature that creates a sort of umbrella under which multiple processes can run on the GPU concurrently and without strict time slicing.
  - merges the contexts of the processes into one scheduler context
- With MPS, the GPU can execute kernels from different processes at the same time as long as the GPU resources (streaming multiprocessors, tensor cores, etc.) are available
  - overlapping
  - an active thread percentage per client. This limits how many streaming multiprocessors (GPU cores, essentially) a client can use. This can be useful if you want to guarantee quality of service (QoS) where two jobs, for example, each get at most 50% of the GPUâ€™s execution resources. If not explicitly set, the jobs will just compete and use whatever GPU resources they can.


A1

1.running an MPS control daemon 
- nvidia-cuda-mps-control
- which then launches an MPS server process that brokers GPU access
- start the MPS server on a node - often one per GPU or one per user
2.run your GPU jobs with an environment variable that connects them to MPS
3.All jobs under that server will share the GPU concurrently


R1

donâ€™t pay the full cost of switching and idling between independent processes.
 
ç¼ºç‚¹

- Note that MPS does not partition GPU memory, so all processes will share the full GPU memory space. MPS is mainly about compute sharing and scheduling. The issue is that one process could request a massive amount of GPU RAM, cause an out-of-memory (OOM) error on the GPU, and result in terminating all of the other processes running on the GPU. This is very disruptive. 
- Another limitation of MPS is that, by default, all MPS clients must run as the same Unix user since they share a context.




T2 - partition GPU into a multi-instance GPU(MIG)

- Starting with the NVIDIA A100 Ampere generation, GPUs can be partitioned at the hardware level into multiple instances using multi-instance GPU.
- MIG allows a GPU to be sliced into as many as 7 smaller logical GPUs - each with its own dedicated portion of memory and compute units, or streaming multiprocessors (SMs). 
  - A 192 GB Blackwell B200 GPU can be split into 7 instances of about 27 GB (192 GB / 7 instances) and 20 SMs (140 SMâ€™s / 7 instances) each
- Each instance acts like a separate GPU from the perspective of software since it has its own memory, its own streaming multiprocessors, and even separate engine contexts.

A

- The GPU has to be put into MIG mode
- the slices created, 
- the node rebooted, and then the slices appear as separate devices to the system.
The Kubernetes device plugin will list MIG devices as resources like â€œnvidia.com/mig-2g.10gbâ€ in the case of a 2 GPU slice of 10 GB.


```bash
# å¯ç”¨ GPU 0 çš„ MIG æ¨¡å¼
 nvidia-smi -i 0 -mig 1

# æŸ¥çœ‹å¯ç”¨çš„åˆ†åŒºé…ç½®ï¼ˆProfileï¼‰
nvidia-smi mig -lgip
# Profile ID | GI Size | CI Count | Memory Size
# 9          | 3g      | 3        | 20GB
# 19         | 1g      | 1        | 5GB


# åˆ›å»º GPU å®ä¾‹ï¼ˆGIï¼‰
 nvidia-smi mig -cgi 9 -C -i 0
# -cgi 9 ä½¿ç”¨ Profile ID ä¸º 9 çš„é…ç½®ï¼ˆå¦‚ 3g.20gbï¼‰
# -C     æ‰§è¡Œåˆ›å»ºæ“ä½œï¼ˆCreateï¼‰
# -i 0   æŒ‡å®š GPU ç¼–å·ä¸º 0


# æŸ¥çœ‹å·²åˆ›å»ºçš„å®ä¾‹
nvidia-smi mig -lgi
# æŸ¥çœ‹ MIG å®ä¾‹çš„ UUIDï¼ˆç”¨äºå®¹å™¨ç»‘å®šï¼‰
nvidia-smi -L
# GPU 0: A100-SXM4-40GB (UUID: GPU-xxxx)
#   MIG 3g.20gb Instance (UUID: MIG-xxxx)


# åœ¨ Docker ä¸­ä½¿ç”¨ MIG å®ä¾‹
docker run -d \
  --gpus "device=MIG-UUID" \
  nvidia/cuda:11.0.3-base-ubuntu20.04 \
  tail -f /dev/null



# Kubernetes ä¸­ä½¿ç”¨ MIGï¼ˆç®€è¦ï¼‰
# å®‰è£…device pluginæ”¯æŒé¢å¤–çš„resources
helm repo add nvdp https://nvidia.github.io/k8s-device-plugin
helm repo add nvgfd https://nvidia.github.io/gpu-feature-discovery
helm repo update
export MIG_STRATEGY=single  # æˆ– mixed
helm install nvdp nvdp/nvidia-device-plugin \
  --version=0.7.0 \
  --set migStrategy=${MIG_STRATEGY}
helm install nvgfd nvgfd/gpu-feature-discovery \
  --version=0.2.0 \
  --set migStrategy=${MIG_STRATEGY}


apiVersion: v1
kind: Pod
metadata:
  name: mig-test
spec:
  containers:
  - name: gpu-test
    image: nvcr.io/nvidia/pytorch:23.07-py3
    command: ["nvidia-smi"]
    args: ["-L"]
    resources:
      limits:
        nvidia.com/mig-1g.5gb: 1
```


R2

- Seprate compute
- Seperate memory
- Seperate user


ç¼ºç‚¹

- If one instance is idle, it canâ€™t lend its resources to another as they are hard partitioned. 
- Also, if youâ€™re not using all MIG slices, youâ€™re wasting resources by leaving them fragmented. Itâ€™s important to plan partition sizes to match your workloads.


#### S - dynamic MIG toggle
- run MIG during the day when lots of small training or inferencing experiments are happening
- turn MIG off at night to run big training jobs that use whole GPUs.
- dynamically



T1 - nvidia-mig-parted + crontab
1.å¼€å…³æœ¬èº«ï¼šnvidia-mig-parted
- è¿™æ˜¯ NVIDIA å®˜æ–¹çš„ GPU åˆ†åŒºç®¡ç†å·¥å…·ï¼Œå¯ declaratively å®šä¹‰ MIG æ¨¡æ¿å¹¶åº”ç”¨
- å¯ç”¨ MIG å¹¶åˆ’åˆ†ä¸º 7 ä¸ª 1g.5gb å®ä¾‹
2.è‡ªåŠ¨åˆ‡æ¢ï¼šä½¿ç”¨ cron åœ¨ç™½å¤©å’Œæ™šä¸Šè‡ªåŠ¨åˆ‡æ¢

A1

```bash
version: v1
mig-configs:
  mig-on:
    - devices: all
      mig-enabled: true
      mig-devices:
        "1g.5gb": 7
  mig-off:
    - devices: all
      mig-enabled: false

nvidia-mig-parted apply -f config.yaml -c all-1g.5gb
```

```bash
# ç™½å¤©å¯ç”¨ MIGï¼ˆæ¯å¤©æ—©ä¸Š 8 ç‚¹ï¼‰
0 8 * * * /usr/local/bin/nvidia-mig-parted apply -f /etc/mig/config.yaml -c mig-on

# æ™šä¸Šå…³é—­ MIGï¼ˆæ¯å¤©æ™šä¸Š 8 ç‚¹ï¼‰
0 20 * * * /usr/local/bin/nvidia-mig-parted apply -f /etc/mig/config.yaml -c mig-off
```



T2 - HAMi 
- æ˜¯ä¸€ä¸ª Kubernetes GPU è™šæ‹ŸåŒ–è°ƒåº¦æ’ä»¶ï¼Œæ”¯æŒ è‡ªåŠ¨æ ¹æ®ä»»åŠ¡ç±»å‹åŠ¨æ€è°ƒæ•´ MIG æ¨¡æ¿

A2

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: hami-device-plugin
  namespace: kube-system
data:
  nodeconfig: |
    - name: MIG-NODE-A
      operatingmode: mig
      scheduleWindow:
        day: "08:00-20:00"
        night: "20:00-08:00"
      migTemplates:
        day: "1g.5gb x 7"
        night: "disable"
```

R2

- ä¸éœ€è¦æ‰‹åŠ¨æ‰§è¡Œ nvidia-smi æˆ– mig-parted
- æ ¹æ®ä»»åŠ¡èµ„æºè¯·æ±‚è‡ªåŠ¨é€‰æ‹©åˆé€‚çš„ MIG æ¨¡æ¿
- æ”¯æŒç»Ÿä¸€èµ„æºæ± ï¼ˆMIG + é MIG èŠ‚ç‚¹æ··åˆè°ƒåº¦ï¼‰
- æ¨è






#### S - Power and thermal limits, GPU downclock
- GPU Boost which automatically adjusts the core clock within power and thermal limits.
- GPUs may be throttled due to excessive heat caused by previous runs


T - lock the clocks for consistency so that the GPU always runs at a fixed maximum frequency.

A

lock the clocks for consistency so that the GPU always runs at a fixed maximum frequency.

R

run-to-run performance is stable and not subject to variations in power or temperature. 


#### S - No workload/idle, GPU sleep
T - Persistence mode that keeps the GPU driver loaded and the hardware in a ready state even when no application is active. 

A

nvidia-smi -pm 1

R

shaves off job-startup latency and prevents cold start delays


#### S - bit memory error
- For long training or inference jobs on huge models, a single memory error could crash the job completely or, even worse, silently corrupt your model without a warning. 

T - Error Correcting Code (ECC) memory on GPUs

- if thereâ€™s a single-bit memory error caused by cosmic rays, ensure the memory can be corrected on the fly
- if thereâ€™s a double-bit error, the error is detected and will throw an error to the calling code. 
- ECC is always enabled and cannot be disabled. 

R

memory-error protection ensure reliability
