---
author: "Hongyao Tang"
title: "5.1 [Align] RLHF"
date: "2025-07-04"
tags: [
    "DL",
]
ShowToc: true
weight: 8
draft: false
---

## LLM development flow
| 学习阶段 | Pretraining | SFT | Align - RLHF 
|------|----------------------------|------------------------------------|--------------------------------------------------------|
| **目标** | 学习语言结构、常识、语义关系 | 让模型学会“如何正确回答问题” | 通过强化学习优化模型，使其输出更符合人类偏好 |
| **数据** | 海量网页、书籍、代码等 | 人工构造的问答对、对话、代码任务等 | 人类对多个模型回答进行排序，用于训练奖励模型 |
| **方法 / 步骤** | 自监督语言建模（预测下一个词） | 标准监督学习 | 1. 训练奖励模型（Reward Model）<br>2. 使用 RL（如 PPO）优化输出 |


## RLHF)


![alt text](images/rlhf.png)









R
| 模型                     | 是否使用SFT<br>(用人工问答对训练模型学会“如何回答问题”) | 是否使用RLHF<br>(优化模型输出，使其更符合人类偏好) | 说明                                                                 |
|--------------------------|---------------|----------------|----------------------------------------------------------------------|
| GPT-1 / GPT-2 / GPT-3 | ❌            | ❌             | 仅使用大规模无监督预训练（语言建模）                                |
| GPT-3.5 | ✅            | ✅             | 使用 SFT + 奖励模型 + RLHF（PPO）进行对齐训练                        |
| GPT-4 / GPT-4-turbo   | ✅            | ✅             | 同样使用 SFT + RLHF，训练过程更复杂，可能加入 DPO 等新技术          |
| ChatGPT（所有版本）   | ✅            | ✅             | ChatGPT 是在 GPT-3.5 / GPT-4 基础上，经过 SFT + RLHF 微调得到的对话模型 |



