---
author: "Hongyao Tang"
title: "5.1 [RL] RL, RLHF, Pure RL"
date: "2025-07-04"
description: "RL, RLHF, Pure RL"
tags: [
    "DL",
]
ShowToc: true
weight: 8
draft: false
---


| 学习阶段 | 1 预训练（Pretraining） | 2 SFT（Supervised Fine-Tuning） | 3 RLHF（Reinforcement Learning with Human Feedback） |
|------|----------------------------|------------------------------------|--------------------------------------------------------|
| **目标** | 学习语言结构、常识、语义关系 | 让模型学会“如何回答问题” | 让模型更符合人类偏好 |
| **数据** | 海量网页、书籍、代码等 | 人工构造的问答对、对话、代码任务等 | 人类对多个回答进行排序 |
| **方法 / 步骤** | 自监督语言建模（预测下一个词） | 标准监督学习 | 1. 训练奖励模型（Reward Model）<br>2. 使用 RL（如 PPO）优化输出 |




| 模型                     | 是否使用SFT<br>(用人工问答对训练模型学会“如何回答问题”) | 是否使用RLHF<br>(优化模型输出，使其更符合人类偏好) | 说明                                                                 |
|--------------------------|---------------|----------------|----------------------------------------------------------------------|
| GPT-1 / GPT-2 / GPT-3 | ❌            | ❌             | 仅使用大规模无监督预训练（语言建模）                                |
| GPT-3.5 | ✅            | ✅             | 使用 SFT + 奖励模型 + RLHF（PPO）进行对齐训练                        |
| GPT-4 / GPT-4-turbo   | ✅            | ✅             | 同样使用 SFT + RLHF，训练过程更复杂，可能加入 DPO 等新技术          |
| ChatGPT（所有版本）   | ✅            | ✅             | ChatGPT 是在 GPT-3.5 / GPT-4 基础上，经过 SFT + RLHF 微调得到的对话模型 |


| 项目 | RL（Reinforcement Learning） | RLHF（Reinforcement Learning with Human Feedback） | Pure RL（纯强化学习） |
|------|------------------------------|----------------------------------------------------|------------------------|
| 是否使用人类标注 | ❌ 不一定 | ✅ 使用人类偏好排序或打分 | ❌ 完全不依赖人工标注 |
| 奖励来源 | 环境反馈（如得分、成功率） | 人类偏好训练出的奖励模型 | 程序化规则、自动评分函数 |
| 是否结合监督学习 | 可选 | 通常在 SFT 后进行 | ❌ 不依赖 SFT，直接 RL |
| 应用场景 | 游戏、机器人、推荐系统等 | 对齐语言模型与人类偏好（如 ChatGPT） | 训练推理模型（如 DeepSeek R1-Zero） |
| 优势 | 强策略优化能力 | 高质量对齐人类意图 | 高度自动化、无需人工参与 |
| 代表方法 | PPO、DQN、A3C 等 | PPO + 奖励模型（Reward Model） | PPO + 自动奖励函数（如格式检查、逻辑验证） |
